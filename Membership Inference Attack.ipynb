{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9466509c",
      "metadata": {
        "id": "9466509c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet18\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Model and data parameters\n",
        "mean = [0.2980, 0.2962, 0.2987]\n",
        "std = [0.2886, 0.2875, 0.2889]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7d457b1",
      "metadata": {
        "id": "f7d457b1"
      },
      "outputs": [],
      "source": [
        "class TaskDataset(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "        self.ids = []\n",
        "        self.imgs = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        id_ = self.ids[index]\n",
        "        img = self.imgs[index]\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        label = self.labels[index]\n",
        "        return id_, img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93ea6ed3",
      "metadata": {
        "id": "93ea6ed3"
      },
      "outputs": [],
      "source": [
        "class MembershipDataset(TaskDataset):\n",
        "    def __init__(self, transform=None):\n",
        "        super().__init__(transform)\n",
        "        self.membership = []\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        id_, img, label = super().__getitem__(index)\n",
        "        return id_, img, label, self.membership[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2051f3f",
      "metadata": {
        "id": "f2051f3f"
      },
      "outputs": [],
      "source": [
        "class AdvancedMembershipInferenceAttack:\n",
        "    def __init__(self, target_model_path=\"./01_MIA.pt\", device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.target_model = self.load_target_model(target_model_path)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std)\n",
        "        ])\n",
        "\n",
        "    def load_target_model(self, model_path):\n",
        "        \"\"\"Load the target ResNet18 model\"\"\"\n",
        "        model = resnet18()\n",
        "        model.fc = torch.nn.Linear(512, 44)\n",
        "        ckpt = torch.load(model_path, map_location=self.device)\n",
        "        model.load_state_dict(ckpt)\n",
        "        model.to(self.device)\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def run_advanced_attack(self, public_dataset_path=\"./pub.pt\", private_dataset_path=\"./priv_out.pt\"):\n",
        "        \"\"\"Run the advanced membership inference attack\"\"\"\n",
        "        print(\"Loading and transforming datasets...\")\n",
        "        # Load datasets\n",
        "        public_data = torch.load(public_dataset_path, weights_only=False, map_location=self.device)\n",
        "        public_data.transform = self.transform\n",
        "\n",
        "        private_data = torch.load(private_dataset_path, weights_only=False, map_location=self.device)\n",
        "        private_data.transform = self.transform\n",
        "\n",
        "        print(\"Extracting comprehensive features from target model using Public Data...\")\n",
        "        public_features = self.extract_comprehensive_features(public_data)\n",
        "        print(\"Extracting comprehensive features from target model using Private Data...\")\n",
        "        private_features = self.extract_comprehensive_features(private_data)\n",
        "\n",
        "        print(\"Training advanced shadow models...\")\n",
        "        # Train shadow models\n",
        "        shadow_models = self.train_shadow_models(public_data, num_models=6)\n",
        "\n",
        "        print(\"Extracting shadow model features using Public Data...\")\n",
        "        # Extract shadow features (simplified for efficiency)\n",
        "        public_shadow_features = self.extract_shadow_features_fast(public_data, shadow_models)\n",
        "        print(\"Extracting shadow model features using Private Data...\")\n",
        "        private_shadow_features = self.extract_shadow_features_fast(private_data, shadow_models)\n",
        "\n",
        "        # Combine all features\n",
        "        public_all_features = np.hstack([public_features, public_shadow_features])\n",
        "        private_all_features = np.hstack([private_features, private_shadow_features])\n",
        "\n",
        "        print(\"Training advanced attack models...\")\n",
        "        # Train attack models\n",
        "        public_labels = np.array(public_data.membership)\n",
        "        attack_models = self.train_advanced_attack_models(public_all_features, public_labels)\n",
        "\n",
        "        print(\"Predicting membership with advanced ensemble...\")\n",
        "        # Predict membership\n",
        "        membership_scores = self.predict_membership_advanced(private_all_features, attack_models)\n",
        "\n",
        "        return private_data.ids, membership_scores\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        filtered_batch = []\n",
        "        for sample in batch:\n",
        "            if sample[0] is None or sample[1] is None or sample[2] is None:\n",
        "                print(\"Skipping invalid sample:\", sample)\n",
        "                continue\n",
        "            filtered_batch.append(sample[:3])\n",
        "        return torch.utils.data.default_collate(filtered_batch)\n",
        "\n",
        "    def compute_gradient_features(self, imgs, labels):\n",
        "        \"\"\"Compute gradient-based features\"\"\"\n",
        "        imgs = imgs.clone().detach().requires_grad_(True)\n",
        "        imgs = imgs.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        logits = self.target_model(imgs)\n",
        "        loss = F.cross_entropy(logits, labels, reduction='none')\n",
        "\n",
        "        gradient_features = []\n",
        "        for i in range(len(imgs)):\n",
        "            # Compute gradients\n",
        "            self.target_model.zero_grad()\n",
        "            loss[i].backward(retain_graph=True)\n",
        "\n",
        "            if imgs.grad is not None:\n",
        "                grad_norm = torch.norm(imgs.grad[i]).item()\n",
        "                grad_mean = torch.mean(torch.abs(imgs.grad[i])).item()\n",
        "                grad_std = torch.std(imgs.grad[i]).item()\n",
        "                gradient_features.append([grad_norm, grad_mean, grad_std])\n",
        "            else:\n",
        "                gradient_features.append([0.0, 0.0, 0.0])\n",
        "\n",
        "        return np.array(gradient_features)\n",
        "\n",
        "    def compute_layer_activations(self, imgs):\n",
        "        \"\"\"Extract intermediate layer activations\"\"\"\n",
        "        activations = {}\n",
        "        imgs = imgs.to(self.device)\n",
        "\n",
        "        def hook_fn(name):\n",
        "            def hook(module, input, output):\n",
        "                activations[name] = output.detach()\n",
        "            return hook\n",
        "\n",
        "        # Register hooks for key layers\n",
        "        handles = []\n",
        "        handles.append(self.target_model.layer1.register_forward_hook(hook_fn('layer1')))\n",
        "        handles.append(self.target_model.layer2.register_forward_hook(hook_fn('layer2')))\n",
        "        handles.append(self.target_model.layer3.register_forward_hook(hook_fn('layer3')))\n",
        "        handles.append(self.target_model.layer4.register_forward_hook(hook_fn('layer4')))\n",
        "\n",
        "        # Forward pass\n",
        "        _ = self.target_model(imgs)\n",
        "\n",
        "        # Remove hooks\n",
        "        for handle in handles:\n",
        "            handle.remove()\n",
        "\n",
        "        # Compute activation statistics\n",
        "        activation_features = []\n",
        "        for i in range(len(imgs)):\n",
        "            features = []\n",
        "            for layer_name in ['layer1', 'layer2', 'layer3', 'layer4']:\n",
        "                if layer_name in activations:\n",
        "                    act = activations[layer_name][i]\n",
        "                    # Global average pooling\n",
        "                    act_pooled = F.adaptive_avg_pool2d(act.unsqueeze(0), (1, 1)).squeeze()\n",
        "\n",
        "                    # Statistical features\n",
        "                    features.extend([\n",
        "                        torch.mean(act_pooled).item(),\n",
        "                        torch.std(act_pooled).item(),\n",
        "                        torch.max(act_pooled).item(),\n",
        "                        torch.min(act_pooled).item()\n",
        "                    ])\n",
        "            activation_features.append(features)\n",
        "\n",
        "        return np.array(activation_features)\n",
        "\n",
        "    def compute_advanced_confidence_features(self, logits, probs, labels):\n",
        "        \"\"\"Compute advanced confidence-based features\"\"\"\n",
        "        features = []\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        for i in range(len(logits)):\n",
        "            logit = logits[i]\n",
        "            prob = probs[i]\n",
        "            label = labels[i]\n",
        "\n",
        "            sample_features = []\n",
        "\n",
        "            # Basic confidence features\n",
        "            max_prob = torch.max(prob).item()\n",
        "            max_logit = torch.max(logit).item()\n",
        "            sample_features.extend([max_prob, max_logit])\n",
        "\n",
        "            # Entropy variations\n",
        "            entropy = -torch.sum(prob * torch.log(prob + 1e-8)).item()\n",
        "            sample_features.append(entropy)\n",
        "\n",
        "            # Temperature scaling effects\n",
        "            for temp in [0.5, 2.0, 5.0]:\n",
        "                temp_probs = F.softmax(logit / temp, dim=0)\n",
        "                temp_entropy = -torch.sum(temp_probs * torch.log(temp_probs + 1e-8)).item()\n",
        "                sample_features.append(temp_entropy)\n",
        "\n",
        "            # Logit statistics\n",
        "            logit_mean = torch.mean(logit).item()\n",
        "            logit_std = torch.std(logit).item()\n",
        "            logit_max_minus_mean = (torch.max(logit) - logit_mean).item()\n",
        "            sample_features.extend([logit_mean, logit_std, logit_max_minus_mean])\n",
        "\n",
        "            # Top-k analysis\n",
        "            top_k_probs, top_k_indices = torch.topk(prob, min(5, len(prob)))\n",
        "            for k in range(len(top_k_probs)):\n",
        "                sample_features.append(top_k_probs[k].item())\n",
        "\n",
        "            # Pad if necessary\n",
        "            while len(top_k_probs) < 5:\n",
        "                sample_features.append(0.0)\n",
        "\n",
        "            # True class analysis\n",
        "            true_class_prob = prob[label].item()\n",
        "            true_class_logit = logit[label].item()\n",
        "            true_class_rank = (prob > true_class_prob).sum().item() + 1\n",
        "            sample_features.extend([true_class_prob, true_class_logit, true_class_rank])\n",
        "\n",
        "            # Margin analysis\n",
        "            sorted_probs, _ = torch.sort(prob, descending=True)\n",
        "            if len(sorted_probs) >= 2:\n",
        "                margin1 = (sorted_probs[0] - sorted_probs[1]).item()\n",
        "                sample_features.append(margin1)\n",
        "            else:\n",
        "                sample_features.append(0.0)\n",
        "\n",
        "            if len(sorted_probs) >= 3:\n",
        "                margin2 = (sorted_probs[1] - sorted_probs[2]).item()\n",
        "                sample_features.append(margin2)\n",
        "            else:\n",
        "                sample_features.append(0.0)\n",
        "\n",
        "            # Prediction correctness\n",
        "            pred_class = torch.argmax(logit).item()\n",
        "            is_correct = 1.0 if pred_class == label.item() else 0.0\n",
        "            sample_features.append(is_correct)\n",
        "\n",
        "            # Loss-based features\n",
        "            loss = F.cross_entropy(logit.unsqueeze(0), label.unsqueeze(0)).item()\n",
        "            sample_features.append(loss)\n",
        "\n",
        "            features.append(sample_features)\n",
        "\n",
        "        return np.array(features)\n",
        "\n",
        "    def extract_comprehensive_features(self, dataset, batch_size=32):\n",
        "        \"\"\"Extract all types of features\"\"\"\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
        "\n",
        "        all_features = []\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(dataloader,desc=\"Extracting features: \")):\n",
        "            if len(batch) == 4:\n",
        "                ids, imgs, labels, _ = batch\n",
        "            else:\n",
        "                ids, imgs, labels = batch\n",
        "\n",
        "            imgs = imgs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Get model predictions\n",
        "                logits = self.target_model(imgs)\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "\n",
        "            # Extract different types of features\n",
        "            confidence_features = self.compute_advanced_confidence_features(logits, probs, labels)\n",
        "\n",
        "            # Gradient features (requires gradients)\n",
        "            gradient_features = self.compute_gradient_features(imgs.clone(), labels)\n",
        "\n",
        "            # Activation features\n",
        "            with torch.no_grad():\n",
        "                activation_features = self.compute_layer_activations(imgs)\n",
        "\n",
        "            # Combine all features\n",
        "            batch_features = np.hstack([\n",
        "                confidence_features,\n",
        "                gradient_features,\n",
        "                activation_features\n",
        "            ])\n",
        "\n",
        "            all_features.append(batch_features)\n",
        "\n",
        "        return np.vstack(all_features)\n",
        "\n",
        "    def train_shadow_models(self, public_dataset, num_models=8):\n",
        "        \"\"\"Train more diverse shadow models\"\"\"\n",
        "        shadow_models = []\n",
        "        total_samples = len(public_dataset)\n",
        "\n",
        "        for i in range(num_models):\n",
        "            print(f\"Training shadow model {i+1}/{num_models}\")\n",
        "\n",
        "            # Create different training strategies\n",
        "            if i % 2 == 0:\n",
        "                # Random subset\n",
        "                subset_size = int(total_samples * (0.4 + 0.3 * np.random.random()))\n",
        "                indices = np.random.choice(total_samples, subset_size, replace=False)\n",
        "            else:\n",
        "                # Class-balanced subset\n",
        "                subset_size = int(total_samples * 0.5)\n",
        "                indices = self.get_balanced_subset(public_dataset, subset_size)\n",
        "\n",
        "            subset_data = torch.utils.data.Subset(public_dataset, indices)\n",
        "\n",
        "            # Train with different hyperparameters\n",
        "            lr = 0.001 * (0.5 + np.random.random())\n",
        "            epochs = 3 + np.random.randint(0, 5)\n",
        "\n",
        "            shadow_model = self.train_single_shadow_model(subset_data, lr=lr, epochs=epochs)\n",
        "            shadow_models.append(shadow_model)\n",
        "\n",
        "        return shadow_models\n",
        "\n",
        "    def get_balanced_subset(self, dataset, subset_size):\n",
        "        \"\"\"Get a class-balanced subset\"\"\"\n",
        "        # Group indices by class\n",
        "        class_indices = {}\n",
        "        for idx in range(len(dataset)):\n",
        "            if hasattr(dataset, 'membership'):\n",
        "                _, _, label, _ = dataset[idx]\n",
        "            else:\n",
        "                _, _, label = dataset[idx]\n",
        "\n",
        "            label = label.item() if torch.is_tensor(label) else label\n",
        "            if label not in class_indices:\n",
        "                class_indices[label] = []\n",
        "            class_indices[label].append(idx)\n",
        "\n",
        "        # Sample from each class\n",
        "        selected_indices = []\n",
        "        samples_per_class = subset_size // len(class_indices)\n",
        "\n",
        "        for class_label, indices in class_indices.items():\n",
        "            if len(indices) >= samples_per_class:\n",
        "                selected = np.random.choice(indices, samples_per_class, replace=False)\n",
        "            else:\n",
        "                selected = indices\n",
        "            selected_indices.extend(selected)\n",
        "\n",
        "        return np.array(selected_indices[:subset_size])\n",
        "\n",
        "    def train_single_shadow_model(self, train_data, lr=0.001, epochs=5):\n",
        "        \"\"\"Train shadow model with advanced techniques\"\"\"\n",
        "        model = resnet18()\n",
        "        model.fc = torch.nn.Linear(512, 44)\n",
        "        model.to(self.device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "        dataloader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            for batch in dataloader:\n",
        "                if len(batch) == 4:\n",
        "                    _, imgs, labels, _ = batch\n",
        "                else:\n",
        "                    _, imgs, labels = batch\n",
        "\n",
        "                imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
        "\n",
        "                # Data augmentation\n",
        "                if np.random.random() > 0.5:\n",
        "                    imgs = torch.flip(imgs, dims=[3])  # Horizontal flip\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def extract_shadow_features_fast(self, dataset, shadow_models):\n",
        "        \"\"\"Fast shadow feature extraction\"\"\"\n",
        "        dataloader = DataLoader(dataset, batch_size=64, shuffle=False, collate_fn=self.collate_fn)\n",
        "        all_shadow_features = []\n",
        "\n",
        "        for model_idx, shadow_model in enumerate(shadow_models):\n",
        "            print(f\"Extracting from shadow model {model_idx + 1}\")\n",
        "\n",
        "            model_features = []\n",
        "            with torch.no_grad():\n",
        "                for batch in dataloader:\n",
        "                    if len(batch) == 4:\n",
        "                        _, imgs, labels, _ = batch\n",
        "                    else:\n",
        "                        _, imgs, labels = batch\n",
        "\n",
        "                    imgs = imgs.to(self.device)\n",
        "                    labels = labels.to(self.device)\n",
        "\n",
        "                    logits = shadow_model(imgs)\n",
        "                    probs = F.softmax(logits, dim=1)\n",
        "\n",
        "                    # Key shadow features\n",
        "                    max_probs = torch.max(probs, dim=1)[0]\n",
        "                    entropies = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
        "                    losses = F.cross_entropy(logits, labels, reduction='none')\n",
        "\n",
        "                    # True class probabilities\n",
        "                    true_class_probs = probs.gather(1, labels.unsqueeze(1)).squeeze()\n",
        "\n",
        "                    batch_features = torch.stack([\n",
        "                        max_probs,\n",
        "                        entropies,\n",
        "                        losses,\n",
        "                        true_class_probs\n",
        "                    ], dim=1)\n",
        "\n",
        "                    model_features.append(batch_features.cpu().numpy())\n",
        "\n",
        "            all_shadow_features.append(np.vstack(model_features))\n",
        "\n",
        "        return np.hstack(all_shadow_features)\n",
        "\n",
        "    def train_advanced_attack_models(self, features, labels):\n",
        "        \"\"\"Train sophisticated ensemble of attack models\"\"\"\n",
        "        # Multiple scalers for robustness\n",
        "        scalers = [\n",
        "            StandardScaler(),\n",
        "            RobustScaler(),\n",
        "        ]\n",
        "\n",
        "        models = []\n",
        "\n",
        "        for i, scaler in enumerate(scalers):\n",
        "            features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "            # Random Forest with optimized parameters\n",
        "            rf = RandomForestClassifier(\n",
        "                n_estimators=500,\n",
        "                max_depth=15,\n",
        "                min_samples_split=10,\n",
        "                min_samples_leaf=5,\n",
        "                max_features='sqrt',\n",
        "                bootstrap=True,\n",
        "                random_state=42 + i\n",
        "            )\n",
        "            rf.fit(features_scaled, labels)\n",
        "            print(\"Trained Random Forest with scalar \", i)\n",
        "            models.append((rf, scaler))\n",
        "\n",
        "            # Gradient Boosting\n",
        "            gb = GradientBoostingClassifier(\n",
        "                n_estimators=300,\n",
        "                learning_rate=0.05,\n",
        "                max_depth=8,\n",
        "                min_samples_split=10,\n",
        "                min_samples_leaf=5,\n",
        "                random_state=42 + i\n",
        "            )\n",
        "            gb.fit(features_scaled, labels)\n",
        "            print(\"Trained Gradient Boosting with scalar \", i)\n",
        "            models.append((gb, scaler))\n",
        "\n",
        "            # SVM with RBF kernel\n",
        "            svm = SVC(\n",
        "                C=10.0,\n",
        "                kernel='rbf',\n",
        "                gamma='scale',\n",
        "                probability=True,\n",
        "                random_state=42 + i\n",
        "            )\n",
        "            svm.fit(features_scaled, labels)\n",
        "            print(\"Trained SVC with scalar \", i)\n",
        "            models.append((svm, scaler))\n",
        "\n",
        "            # Neural Network\n",
        "            mlp = MLPClassifier(\n",
        "                hidden_layer_sizes=(256, 128, 64),\n",
        "                activation='relu',\n",
        "                solver='adam',\n",
        "                alpha=0.001,\n",
        "                learning_rate_init=0.001,\n",
        "                max_iter=500,\n",
        "                random_state=42 + i\n",
        "            )\n",
        "            mlp.fit(features_scaled, labels)\n",
        "            print(\"Trained Multi Level Perceptron with scalar \", i)\n",
        "            models.append((mlp, scaler))\n",
        "\n",
        "        return models\n",
        "\n",
        "    def predict_membership_advanced(self, features, models):\n",
        "        \"\"\"Advanced ensemble prediction with confidence weighting\"\"\"\n",
        "        predictions = []\n",
        "        weights = []\n",
        "\n",
        "        for model, scaler in models:\n",
        "            features_scaled = scaler.transform(features)\n",
        "\n",
        "            if hasattr(model, 'predict_proba'):\n",
        "                pred = model.predict_proba(features_scaled)[:, 1]\n",
        "            else:\n",
        "                pred = model.decision_function(features_scaled)\n",
        "                # Normalize to [0, 1]\n",
        "                pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
        "\n",
        "            predictions.append(pred)\n",
        "\n",
        "            # Weight based on model type (some models are more reliable)\n",
        "            if isinstance(model, (RandomForestClassifier, GradientBoostingClassifier)):\n",
        "                weights.append(1.5)\n",
        "            elif isinstance(model, SVC):\n",
        "                weights.append(1.2)\n",
        "            else:\n",
        "                weights.append(1.0)\n",
        "\n",
        "        # Weighted ensemble\n",
        "        predictions = np.array(predictions)\n",
        "        weights = np.array(weights)\n",
        "        weights = weights / weights.sum()\n",
        "\n",
        "        ensemble_pred = np.average(predictions, axis=0, weights=weights)\n",
        "\n",
        "        # Apply calibration/sharpening\n",
        "        ensemble_pred = np.power(ensemble_pred, 0.8)  # Slight sharpening\n",
        "\n",
        "        return ensemble_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9f2c183",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9f2c183",
        "outputId": "979b4daa-de60-411a-91bc-53644a9f45ad"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Initialize advanced attack\n",
        "    attack = AdvancedMembershipInferenceAttack()\n",
        "    ids, scores = attack.run_advanced_attack()\n",
        "\n",
        "    # Prepare submission\n",
        "    df = pd.DataFrame({\n",
        "        \"ids\": ids,\n",
        "        \"score\": scores\n",
        "    })\n",
        "\n",
        "    df.to_csv(\"test.csv\", index=None)\n",
        "\n",
        "    # Submit results (replace TOKEN with your actual token)\n",
        "    response = requests.post(\n",
        "        \"http://34.122.51.94:9090/mia\",\n",
        "        files={\"file\": open(\"test.csv\", \"rb\")},\n",
        "        headers={\"token\": \"96005201\"}\n",
        "    )\n",
        "    print(response.json())\n",
        "\n",
        "    print(f\"Attack completed. Submission saved to test.csv\")\n",
        "    print(f\"Score statistics: Mean={np.mean(scores):.3f}, Std={np.std(scores):.3f}\")\n",
        "    print(f\"Score range: [{np.min(scores):.3f}, {np.max(scores):.3f}]\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pve-nPLt62cU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pve-nPLt62cU",
        "outputId": "a7e3926b-c744-4148-b069-c776154cb49d"
      },
      "outputs": [],
      "source": [
        "response = requests.post(\n",
        "        \"http://34.122.51.94:9090/mia\",\n",
        "        files={\"file\": open(\"/content/test.csv\", \"rb\")},\n",
        "        headers={\"token\": \"96005201\"}\n",
        "    )\n",
        "print(response.json())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
